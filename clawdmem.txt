How Clawdbot Remembers Everything
Clawdbot is an open-source personal AI assistant (MIT licensed) created by Peter Steinberger that has quickly gained traction with over 32,600 stars on GitHub at the time of writing this blog. Unlike ChatGPT or Claude, which run in the cloud, Clawdbot runs locally on your machine and integrates with chat platforms you already use, like Discord, WhatsApp, Telegram, and more.
What sets Clawdbot apart is its ability to handle real-world tasks autonomously: managing emails, scheduling calendar events, handling flight check-ins, and running background jobs on a schedule. But what caught my attention was its persistent memory system, which maintains 24/7 context retention, remembering conversations and building upon previous interactions indefinitely.
If youâ€™ve read my previous posts on ChatGPT memory and Claude memory, you know I am fascinated by how different AI products approach memory. Clawdbot takes a fundamentally different approach: instead of cloud-based, company controlled memory, it keeps everything local, giving users full ownership of their context and skills.
Letâ€™s dive into how it works.
How Context is Built
Before diving into memory, letâ€™s understand what the model sees on each request:
[0] System Prompt (static + conditional instructions)
[1] Project Context (bootstrap files: AGENTS.md, SOUL.md, etc.)
[2] Conversation History (messages, tool calls, compaction summaries)
[3] Current Message
The system prompt defines the agentâ€™s capabilities and available tools. Whatâ€™s relevant for memory is Project Context, which includes user-editable Markdown files injected into every request:
These files live in the agentâ€™s workspace alongside memory files, making the entire agent configuration transparent and editable.
Context vs Memory
Understanding the distinction between context and memory is fundamental to understanding Clawdbot.
Context is everything the model sees for a single request:
Context = System Prompt + Conversation History + Tool Results + Attachments
Context is:
Ephemeral - exists only for this request
Bounded - limited by the modelâ€™s context window (e.g., 200K tokens)
Expensive - every token counts toward API costs and speed
Memory is whatâ€™s stored on disk:
Memory = MEMORY.md + memory/*.md + Session Transcripts
Memory is:
Persistent - survives restarts, days, months
Unbounded - can grow indefinitely
Cheap - no API cost to store
Searchable - indexed for semantic retrieval
The Memory Tools
The agent accesses memory through two specialized tools:
1. memory_search
Purpose: Find relevant memories across all files
json
{
  "name": "memory_search",
  "description": "Mandatory recall step: semantically search MEMORY.md + memory/*.md before answering questions about prior work, decisions, dates, people, preferences, or todos",
  "parameters": {
    "query": "What did we decide about the API?",
    "maxResults": 6,
    "minScore": 0.35
  }
}
Returns
json
{
  "results": [
    {
      "path": "memory/2026-01-20.md",
      "startLine": 45,
      "endLine": 52,
      "score": 0.87,
      "snippet": "## API Discussion\nDecided to use REST over GraphQL for simplicity...",
      "source": "memory"
    }
  ],
  "provider": "openai",
  "model": "text-embedding-3-small"
}
2. memory_get
Purpose: Read specific content after finding it
json
{
  "name": "memory_get",
  "description": "Read specific lines from a memory file after memory_search",
  "parameters": {
    "path": "memory/2026-01-20.md",
    "from": 45,
    "lines": 15
  }
}
Returns:
json
{
  "path": "memory/2026-01-20.md",
  "text": "## API Discussion\n\nMet with the team to discuss API architecture.\n\n### Decision\nWe chose REST over GraphQL for the following reasons:\n1. Simpler to implement\n2. Better caching\n3. Team familiarity\n\n### Endpoints\n- GET /users\n- POST /auth/login\n- GET /projects/:id"
}
Writing to Memory
There is no dedicated memory_write tool. The agent writes to memory using the standard write and edit tools which it uses for any file. Since memory is just Markdown, you can manually edit these files too (they will be re-indexed automatically).
The decision of where to write is prompt-driven via AGENTS.md:
Automatic writes also occur during pre-compaction flush and session end (covered in later sections).
Memory Storage
Clawdbotâ€™s memory system is built on the principle that â€œMemory is plain Markdown in the agent workspace.â€
Two-Layer Memory System
Memory lives in the agentâ€™s workspace (default: ~/clawd/):
~/clawd/
â”œâ”€â”€ MEMORY.md              - Layer 2: Long-term curated knowledge
â””â”€â”€ memory/
    â”œâ”€â”€ 2026-01-26.md      - Layer 1: Today's notes
    â”œâ”€â”€ 2026-01-25.md      - Yesterday's notes
    â”œâ”€â”€ 2026-01-24.md      - ...and so on
    â””â”€â”€ ...
Layer 1: Daily Logs (memory/YYYY-MM-DD.md)
These are append-only daily notes that the agent writes here throughout the day. The agent writes this when the agent wants to remember something or when explicitly told to remember something.
# 2026-01-26

## 10:30 AM - API Discussion
Discussed REST vs GraphQL with user. Decision: use REST for simplicity.
Key endpoints: /users, /auth, /projects.

## 2:15 PM - Deployment
Deployed v2.3.0 to production. No issues.

## 4:00 PM - User Preference
User mentioned they prefer TypeScript over JavaScript.
Layer 2: Long-term Memory (MEMORY.md)
This is curated, persistent knowledge. Agent writes to this when significant events, thoughts, decisions, opinions, and lessons are learned.
# Long-term Memory

## User Preferences
- Prefers TypeScript over JavaScript
- Likes concise explanations
- Working on project "Acme Dashboard"

## Important Decisions
- 2026-01-15: Chose PostgreSQL for database
- 2026-01-20: Adopted REST over GraphQL
- 2026-01-26: Using Tailwind CSS for styling

## Key Contacts
- Alice (alice@acme.com) - Design lead
- Bob (bob@acme.com) - Backend engineer
How the Agent Knows to Read Memory
The AGENTS.md file (which is automatically loaded) contains instructions:
## Every Session

Before doing anything else:
1. Read SOUL.md - this is who you are
2. Read USER.md - this is who you are helping
3. Read memory/YYYY-MM-DD.md (today and yesterday) for recent context
4. If in MAIN SESSION (direct chat with your human), also read MEMORY.md

Don't ask permission, just do it.
How Memory Gets Indexed
When you save a memory file, hereâ€™s what happens behind the scenes:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. File Saved                                              â”‚
â”‚     ~/clawd/memory/2026-01-26.md                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. File Watcher Detects Change                             â”‚
â”‚     Chokidar monitors MEMORY.md + memory/**/*.md            â”‚
â”‚     Debounced 1.5 seconds to batch rapid writes             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Chunking                                                â”‚
â”‚     Split into ~400 token chunks with 80 token overlap      â”‚
â”‚                                                             â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                      â”‚
â”‚     â”‚ Chunk 1        â”‚                                      â”‚
â”‚     â”‚ Lines 1-15     â”‚â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚                               â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚ (80 token overlap)            â”‚
â”‚     â”‚ Chunk 2        â”‚â—„â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚     â”‚ Lines 12-28    â”‚â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚                               â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚                               â”‚
â”‚     â”‚ Chunk 3        â”‚â—„â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚     â”‚ Lines 25-40    â”‚                                      â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚
â”‚                                                             â”‚
â”‚     Why 400/80? Balances semantic coherence vs granularity. â”‚
â”‚     Overlap ensures facts spanning chunk boundaries are     â”‚
â”‚     captured in both. Both values are configurable.         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. Embedding                                               â”‚
â”‚     Each chunk -> embedding provider -> vector              â”‚
â”‚                                                             â”‚
â”‚     "Discussed REST vs GraphQL" ->                          â”‚
â”‚         OpenAI/Gemini/Local ->                              â”‚
â”‚         [0.12, -0.34, 0.56, ...]  (1536 dimensions)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. Storage                                                 â”‚
â”‚     ~/.clawdbot/memory/<agentId>.sqlite                     â”‚
â”‚                                                             â”‚
â”‚     Tables:                                                 â”‚
â”‚     - chunks (id, path, start_line, end_line, text, hash)   â”‚
â”‚     - chunks_vec (id, embedding)      -> sqlite-vec         â”‚
â”‚     - chunks_fts (text)               -> FTS5 full-text     â”‚
â”‚     - embedding_cache (hash, vector)  -> avoid re-embedding â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
sqlite-vec is a SQLite extension that enables vector similarity search directly in SQLite, no external vector database required.
FTS5 is SQLiteâ€™s built-in full-text search engine that powers the BM25 keyword matching. Together, they allow Clawdbot to run hybrid search (semantic + keyword) from a single lightweight database file.
How Memory is Searched
When you search memory, Clawdbot runs two search strategies in parallel. Vector search (semantic) finds content that means the same thing and BM25 search (keyword) finds content with exact tokens.
The results are combined with weighted scoring:
finalScore = (0.7 * vectorScore) + (0.3 * textScore)
Why 70/30? Semantic similarity is the primary signal for memory recall, but BM25 keyword matching catches exact terms that vectors might miss (names, IDs, dates). Results below a minScore threshold (default 0.35) are filtered out. All these values are configurable.
This ensures you get good results whether you are searching for concepts (â€œthat database thingâ€) or specifics (â€œPOSTGRES_URLâ€).
Multi-Agent Memory
Clawdbot supports multiple agents, each with complete memory isolation:
~/.clawdbot/memory/              # State directory (indexes)
â”œâ”€â”€ main.sqlite                  # Vector index for "main" agent
â””â”€â”€ work.sqlite                  # Vector index for "work" agent

~/clawd/                         # "main" agent workspace (source files)
â”œâ”€â”€ MEMORY.md
â””â”€â”€ memory/
    â””â”€â”€ 2026-01-26.md

~/clawd-work/                    # "work" agent workspace (source files)
â”œâ”€â”€ MEMORY.md
â””â”€â”€ memory/
    â””â”€â”€ 2026-01-26.md
The Markdown files (source of truth) live in each workspace, while the SQLite indexes (derived data) live in the state directory. Each agent gets its own workspace and index. The memory manager is keyed by agentId + workspaceDir, so no cross-agent memory search happens automatically.
Can agents read each otherâ€™s memories? Not by default. Each agent only sees its own workspace. However, the workspace is a soft sandbox (default working directory), not a hard boundary. An agent could theoretically access another workspace using absolute paths unless you enable strict sandboxing.
This isolation is useful for separating contexts. A â€œpersonalâ€ agent for WhatsApp and a â€œworkâ€ agent for Slack, each with distinct memories and personalities.
Compaction
Every AI model has a context window limit. Claude has 200K tokens, GPT-5.1 has 1M. Long conversations eventually hit this wall.
When that happens, Clawdbot uses compaction: summarizing older conversation into a compact entry while keeping recent messages intact.
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Before Compaction                                          â”‚
â”‚  Context: 180,000 / 200,000 tokens                          â”‚
â”‚                                                             â”‚
â”‚  [Turn 1] User: "Let's build an API"                        â”‚
â”‚  [Turn 2] Agent: "Sure! What endpoints do you need?"        â”‚
â”‚  [Turn 3] User: "Users and auth"                            â”‚
â”‚  [Turn 4] Agent: *creates 500-line schema*                  â”‚
â”‚  [Turn 5] User: "Add rate limiting"                         â”‚
â”‚  [Turn 6] Agent: *modifies code*                            â”‚
â”‚  ... (100 more turns) ...                                   â”‚
â”‚  [Turn 150] User: "What's the status?"                      â”‚
â”‚                                                             â”‚
â”‚  âš ï¸ APPROACHING LIMIT                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Compaction Triggered                                       â”‚
â”‚                                                             â”‚
â”‚  1. Summarize turns 1-140 into a compact summary            â”‚
â”‚  2. Keep turns 141-150 intact (recent context)              â”‚
â”‚  3. Persist summary to JSONL transcript                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  After Compaction                                           â”‚
â”‚  Context: 45,000 / 200,000 tokens                           â”‚
â”‚                                                             â”‚
â”‚  [SUMMARY] "Built REST API with /users, /auth endpoints.    â”‚
â”‚   Implemented JWT auth, rate limiting (100 req/min),        â”‚
â”‚   PostgreSQL database. Deployed to staging v2.4.0.          â”‚
â”‚   Current focus: production deployment prep."               â”‚
â”‚                                                             â”‚
â”‚  [Turn 141-150 preserved as-is]                             â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Automatic vs Manual Compaction
Automatic: Triggers when approaching context limit
You will see: ğŸ§¹ Auto-compaction complete in verbose mode
The original request will retry with compacted context
Manual: Use /compact command
`/compact` Focus on decisions and open questions
Unlike some optimizations, compaction persists to disk. The summary is written to the sessionâ€™s JSONL transcript file, so future sessions start with the compacted history.
The Memory Flush
LLM-based compaction is a lossy process. Important information may be summarized away and potentially lost. To counter that, Clawdbot uses the pre-compaction memory flush.
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Context Approaching Limit                                  â”‚
â”‚                                                             â”‚
â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  75% of context       â”‚
â”‚                              â†‘                              â”‚
â”‚                    Soft threshold crossed                   â”‚
â”‚                    (contextWindow - reserve - softThreshold)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Silent Memory Flush Turn                                   â”‚
â”‚                                                             â”‚
â”‚  System: "Pre-compaction memory flush. Store durable        â”‚
â”‚           memories now (use memory/YYYY-MM-DD.md).          â”‚
â”‚           If nothing to store, reply with NO_REPLY."        â”‚
â”‚                                                             â”‚
â”‚  Agent: reviews conversation for important info           â”‚
â”‚         writes key decisions/facts to memory files        â”‚
â”‚         -> NO_REPLY (user sees nothing)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Compaction Proceeds Safely                                 â”‚
â”‚                                                             â”‚
â”‚  Important information is now on disk                       â”‚
â”‚  Compaction can proceed without losing knowledge            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
The memory flush is configurable in clawdbot.yaml file or clawdbot.json file.
json
{
  agents: {
    defaults: {
      compaction: {
        reserveTokensFloor: 20000,
        memoryFlush: {
          enabled: true,
          softThresholdTokens: 4000,
          systemPrompt: "Session nearing compaction. Store durable memories now.",
          prompt: "Write lasting notes to memory/YYYY-MM-DD.md; reply NO_REPLY if nothing to store."
        }
      }
    }
  }
}
Pruning
Tool results can be huge. A single exec command might output 50,000 characters of logs. Pruning trims these old outputs without rewriting history. It is a lossy process and the old outputs are not recoverable.
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BEFORE PRUNING (in-memory)                                 â”‚
â”‚                                                             â”‚
â”‚  Tool Result (exec): [50,000 chars of npm install output]   â”‚
â”‚  Tool Result (read): [Large config file, 10,000 chars]      â”‚
â”‚  Tool Result (exec): [Build logs, 30,000 chars]             â”‚
â”‚  User: "Did the build succeed?"                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼ (Soft trim + hard clear)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AFTER PRUNING (sent to model)                              â”‚
â”‚                                                             â”‚
â”‚  Tool Result (exec): "npm WARN deprecated...[truncated]     â”‚
â”‚                       ...Successfully installed."           â”‚
â”‚  Tool Result (read): "[Old tool result content cleared]"    â”‚
â”‚  Tool Result (exec): [Kept - too recent to prune]           â”‚
â”‚  User: "Did the build succeed?"                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
JSONL file on disk: UNCHANGED (full outputs still there)
Cache-TTL Pruning
Anthropic caches prompt prefixes for up to 5 minutes to reduce latency and cost on repeated calls. When the same prompt prefix is sent within the TTL window, cached tokens cost ~90% less. After the TTL expires, the next request must re-cache the entire prompt.
The problem: if a session goes idle past the TTL, the next request loses the cache and must re-cache the full conversation history at full â€œcache writeâ€ pricing.
Cache-TTL pruning solves this by detecting when the cache has expired and trimming old tool results before the next request. Smaller prompt to re-cache means lower cost:
json
{
  agent: {
    contextPruning: {
      mode: "cache-ttl",      // Only prune after cache expires
      ttl: "600",              // Match your cacheControlTtl
      keepLastAssistants: 3,  // Protect recent tool results
      softTrim: {
        maxChars: 4000,
        headChars: 1500,
        tailChars: 1500
      },
      hardClear: {
        enabled: true,
        placeholder: "[Old tool result content cleared]"
      }
    }
  }
}
Session Lifecycle
Sessions donâ€™t last forever. They reset based on configurable rules, creating natural boundaries for memory. The default behaviour is reset everyday. But there are other modes available.
Session Memory Hook
When you run /new to start a fresh session, the session memory hook can automatically save context:
/new
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SESSION-MEMORY HOOK TRIGGERED                              â”‚
â”‚                                                             â”‚
â”‚  1. Extract last 15 messages from ending session            â”‚
â”‚  2. Generate descriptive slug via LLM                       â”‚
â”‚  3. Save to ~/clawd/memory/2026-01-26-api-design.md         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NEW SESSION STARTS                                         â”‚
â”‚                                                             â”‚
â”‚  Previous context is now searchable via memory_search       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Conclusion
Clawdbotâ€™s memory system succeeds because it embraces several key principles:
1. Transparency Over Black Boxes
Memory is plain Markdown. You can read it, edit it, and version control it. No opaque databases or proprietary formats.
2. Search Over Injection
Rather than stuffing context with everything, the agent searches for whatâ€™s relevant. This keeps context focused and costs down.
3. Persistence Over Session
Important information survives in files on disk, not just in conversation history. Compaction canâ€™t destroy whatâ€™s already saved.
4. Hybrid Over Pure
Vector search alone misses exact matches. Keyword search alone misses semantics. Hybrid gives you both.
References
Clawdbot Documentation - Official docs covering setup, configuration, and all features
GitHub Repository - Source code, issues, and community contributions
