[Unit]
Description=vLLM OpenAI-Compatible API Server
Documentation=https://docs.vllm.ai
After=network.target

[Service]
Type=simple
User=the_bomb
Group=the_bomb
WorkingDirectory=/home/the_bomb

# GPU visibility - both RTX 3080 cards
Environment="CUDA_VISIBLE_DEVICES=0,1"
Environment="VLLM_ATTENTION_BACKEND=FLASH_ATTN"

# vLLM server with tensor parallelism across 2 GPUs
# Model: Qwen2.5-Coder-14B-Instruct (fits in 20GB total VRAM with TP=2)
ExecStart=/home/the_bomb/venv/bin/python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-Coder-14B-Instruct \
    --tensor-parallel-size 2 \
    --port 8000 \
    --host 0.0.0.0 \
    --max-model-len 8192 \
    --dtype auto \
    --trust-remote-code

# Restart policy
Restart=always
RestartSec=10

# Resource limits
LimitNOFILE=65535

# Logging
StandardOutput=journal
StandardError=journal
SyslogIdentifier=vllm

[Install]
WantedBy=multi-user.target
